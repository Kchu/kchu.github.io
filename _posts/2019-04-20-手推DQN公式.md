---
layout: post
title:  "手推DQN公式"
date:   2019-4-20 20:29:56 +0800
<!-- categories: RL -->
tags: 强化学习 DQN
---

* content
{:toc}

# 1. 强化学习基本公式
基于MDP形成的交互序列，其中从状态到行动的转换可以通过策略确定，而由于环境的原因，从行动到下一个状态的转换有时并不能确定，在衡量价值时候，需要考虑每一种转换带来的影响，这就需要基于状态转换求解长期回报的期望。另$\tau$为根据策略和状态转换采样得到的序列，那么价值的公式定义为：
$$v_{\pi}(s_{t})=E_{s,a\sim \tau}\left [ \gamma^{k}r_{t+k+1} \right ]\Rightarrow
v_{\pi}(s_{t})=E_{\tau}\left [\sum_{k=0}^{\infty }\gamma^{k}r_{t+k+1} \right ]$$

$\tau$是状态$s_{t}$出发的某条路径，我们可以按照马尔科夫决策过程展开。再根据代换消元，得到：

$$ v_{\pi}\left ( s_{t} \right )=\sum_{a_{t}}\pi\left ( a_{t}|s_{t} \right)\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t},a_{t} \right )\left [ r_{t+1} + v\left ( s_{t+1} \right )\right ) ]$$

同样的，状态-行动值函数同样有一个类似的公式：

$$ q_{\pi}\left ( s_{t},a_{t} \right )=\sum_{a_{t}}\pi\left ( a_{t}|s_{t} \right)\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t},a_{t} \right )\left [ r_{t+1} + q_{\pi}\left ( s_{t+1},a_{t+1} \right )\right ) ]$$

# 2.Q-Learning基础
## 2.1 蒙特卡洛
前面已知模型情况下，根据Bellman公式，通过不断迭代，得到状态-动作值函数公式为：

$$q_{\pi}\left ( s_{t},a_{t} \right )=\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t} \right )\left [ r\left (  s_{t}  \right) + \gamma v_{\pi}\left ( s_{t+1} \right )\right ) ]$$

在无模型问题中，状态转移概率无从得知，所以公式转变为：

$$q_{\pi}\left ( s_{t},a_{t} \right )=E_{s_{t+1}\sim p,\pi}\left [ \gamma^{k}r_{t+k} \right ]$$

看到左边的期望，我们考虑用蒙特卡洛的方法通过随即采样估计期望值。假设我们通过一些方法得到大量的样本序列，就可以根据这些样本值估计样本序列，公式近似等于：

$$q(s,a)\simeq \simeq \frac{1}{N} \sum_{i=1}^{N} \sum_{k=0}^{\infty } \gamma^{k}r_{t+k}$$

如果这个序列是有限的，得到公式如下：

$$q(s,a)\simeq  \frac{1}{N} \sum_{i=1}^{N} \left ( r_{1}^{i} + r_{2}^{i} + ...\right )$$

如果我们有一个状态-行为值函数，在通过交互之后得到每一个状态-行动对应的长期回报值，将其交给值函数进行更新，更新公式如下：

$$ q_{t}^{N}=\frac{1}{N}\sum_{i=1}^{N}\hat{q}_{t}^{i}=\frac{1}{N}\left [ \sum_{i=1}^{N-1} \hat{q}_{t}^{i} + \hat{q}_{t}^{N} \right ]
=\frac{1}{N}\left ( N-1 \right ) q_{t}^{N-1} + \frac{1}{N}q_{t}^{N} 
=q_{t}^{N-1}+\frac{1}{N}\left ( \hat{q}_{t}^{N} - q_{t}^{N-1} \right )$$

## 2.2 TD
利用蒙特卡洛方法，状态值函数为：

$$v_{\pi}(s_{t})=E_{s,a\sim \tau}[ r(s')+q(s',a')]=\frac{1}{N}\sum_{i=1}^{N}[r(s'_{i})+q(s'_{i},a'_{i})]$$

TD在估计价值时，使用了当前回报和下一时刻的价值估计，迭代更新开始：

$$q_{t}(s,a)=q_{t-1}(s,a)+\frac{1}{N}\left [ r(s')+q(s',a')-q_{t-1}(s,a) \right ]$$

## 2.3 Q-Learning

$$q^{T}(s,a)=q^{T-1}(s,a)+\frac{1}{N}\left [ r(s')+ max_{a'}q^{T-1}(s',a')-q^{T-1}(s,a) \right ]$$

# 3.Deep Q-Learning
整体思路：在使用Q-Learning作为优化算法的前提下，使用深层神经网络表示值函数，直接使用游戏图像和得分进行训练，不再使用其他领域的知识。
## 3.1 数据预处理
将原始图像转化为84*84维的灰度图，收集从当前时刻起的前N帧画面，并将这些信息结合起来作为模型的输入。实验中N被设置为4。另外，游戏的得分也要作预处理，将不同游戏的得分都压缩到[-1, 1]之间。
### 3.2 环境交互
ε-greedy的策略，一开始策略以100%的概率随机产生行动，随着训练的进行，概率不断衰减，最终衰减至10%，也就是说有90%的概率执行当前最优策略。这样以探索为主的策略转换为以利用为主的策略。
### 3.3 模型结构
模型输出是一个长度为|A|的向量，向量中的每一个值表示对应行动的价值估计，这样只需要计算一次就可以求出所有行动的价值，无论行动有多少，我们评估价值的时间是一样的。模型主体采用的是卷积神经网络。三层卷积网络和一层全连接层。
### 3.4 Random Start
为了增强探索性同时不至于使模型效果变糟，我们可以设定在游戏开始很短的一段时间内，让Agent执行随机的行动。
### 3.5 Frame-Skipping
每隔一定帧数执行一次行动，例如每隔四帧进行一次行动选择。
### 3.6 Replay Buffer
（1）交互得到的序列存在一定的相关性；（2）交互数据的使用效率。采用Replay Buffer保存交互的样本信息，保存当前状态s、行动a和长期累计回报v。Replay Buffer包括两个过程：收集样本和采集样本。Replay Buffer会在缓存中均匀地随机采样进行学习。
### 3.7 Target Network
算法分成如下两个步骤：


1. 计算当前状态行动下的价值目标值：

$$\Delta q(s,a) = r(s') + max_{a'}q^{T-1}(s',a')$$

2. 网络模型的更新

$$q^{T}(s,a)=q^{T-1}(s,a)+\frac{1}{N}\left [ \Delta q(s,a)-q^{T-1}(s,a) \right ]$$

为了避免模型更新带来的波动，引入目标网络，而原本的模型被称为表现模型（Behavior Network），训练过程如下：
1. 训练开始时，两个模型使用完全相同的参数；
2. 在训练过程中，Behavior Network负责与环境交互，得到交互样本；
3. 在学习过程中，由Q-Learning得到的目标价值由Target Network得到，然后用它与Behavior Network的估计值进行比较得出目标值并更新Behavior Network。
4. 每当训练完成一定轮数的迭代，Behavior Network模型的参数就会同步给Target Network，进行下一阶段的学习。

通过Target Network，计算目标价值的模型在一定时间内将被固定，这样模型可以减轻波动性。
