---
layout: post
title:  "无模型的最优价值算法"
date:   2019-01-20 18:37:56 +0800
<!-- categories: RL -->
tags: 无模型 强化学习
---

* content
{:toc}

<!-- <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> -->

# 1. Q-Learning和Deep Q-Learning

无模型（Model Free）策略优化问题。
> 算法思路：

- 确定一个初始策略
- 用这个策略进行游戏，得到一些游戏序列（Epidose）
- 将这些序列聚合起来，得到状态对应的值函数
得到值函数相当于完成了策略评估，继续按照策略迭代的方法进行策略改进的操作，得到更新后的策略。如果更新完成，过程结束，否则回到过程2。

> 学习的关键在于：

如何得到这些游戏序列
如何使用序列进行评估
## 1.1 蒙特卡洛方法

状态-行为值函数在转移概率未知的情况下，公式转变为：

$$ q_{\pi}\left ( s_{t},a_{t} \right )=E_{s_{t+1}\sim  p,\pi}\left [ \sum_{k=0}^{\infty } \gamma^{k}r_{t+k}\right ] $$

蒙特卡洛方法是一种通过随机采样估计期望值的方法，假设我们通过一些方法得到了大量的样本序列： 对应的回报序列是： 。我们可以通过这些样本序列逼近真实的期望值，那么公式可以近似等于：

$$ q\left ( s,a \right )\simeq \frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{\infty }\gamma^{k}r_{t+k} $$

如果这个序列有限，可以直接使用序列产生的回报和作为价值：
 
> 算法过程如下：

- 让Agent与环境交互得到交互序列 
- 通过序列计算出每一时刻的价值
- 将这些价值累积到值函数中进行更新
- 根据更新的值函数更新策略

> 算法实现：两个子算法

- 蒙特卡洛计算：

首先利用当前策略玩游戏，产生一系列的序列；
产生序列之后，按照时间的反方向计算每一个时刻的状态行动和对应的长期回报值；
最后将每一个状态-行动对应的长期回报值交给值函数进行更新。 

$$ q_{t}^{N}=\frac{1}{N}\sum_{i=1}^{N}\hat{q}_{t}^{i}=\frac{1}{N}\left [ \sum_{i=1}^{N-1} \hat{q}_{t}^{i} + \hat{q}_{t}^{N} \right ]$$
$$ =\frac{1}{N}\left ( N-1 \right ) q_{t}^{N-1} + \frac{1}{N}q_{t}^{N} $$
$$ =q_{t}^{N-1}+\frac{1}{N}\left ( \hat{q}_{t}^{N} - q_{t}^{N-1} \right ) $$
- 策略提升：

通过该值函数计算状态-行为值函数 ，然后根据同一状态下的行为价值更新策略，完成状态的更新。

$$ q_{\pi}\left ( s_{t},a_{t} \right )=\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t} \right )\left [ r_{t+1} + \gamma v\left ( s_{t+1} \right )\right ) ] $$
$$ \large \pi\left( s \right )=argmax_{a}q\left( s,a \right ) $$

策略提升同上面基于模型的方法
## 1.2 $ε-greedy$准则

　　在无模型的情况下，我们要尽可能地让每一个状态下的行动都有概率执行，所以要改进策略模块，采用ε-greedy算法。首先随机生成一个的数，然后用这个随机数判断，如果随机数小于某个值ε，就采用完全随机的方式产生行动，此时每个行动产生的概率是一样的；如果随机数大于某个值ε，就选择当前的最优策略。以此完善遍历到所有状态。

　　探索与利用：探索，选择一些不同于当前策略的行动，利用就是持续使用当前的最优策略，尽可能地获得更多的回报。$ε-greedy$使得两种策略达到平衡。
## 1.3 蒙特卡洛方法的方差问题

蒙特卡洛方法的方差较大，不管是采用every-visit还是first-visit，方差都是很大。
## 1.4 时序差分法

　　时序差分法（Temporal Difference，TD），结合蒙特卡洛方法和动态规划法。时间差分⽅法结合了蒙特卡罗的采样⽅法（即做试验）和动态规划⽅法的bootstrapping（利⽤后继状态的值函数估计当前值函数）。前面提到的状态-行为值函数：

$$ q_{\pi}\left ( s,a \right )=\sum_{s'}p\left ( s'|s,a \right )\left [ r(s') + \sum_{a'} \pi\left ( a'|s' \right ) q\left ( s'|a' \right )\right ] $$

那么利用蒙特卡洛方法，可以将公式变为：

$$ q\left ( s,a \right )=E_{(s,a)\sim \pi,p}\left [ r(s') + q\left ( s'|a' \right )\right ] $$

与其不同的是，TD法使用了当前回报和下一时刻的价值估成迭代更新的形式：

$$ q_{t}\left ( s,a \right )=q_{t-1}\left ( s,a \right )+\frac{1}{N}\left [ r(s')+q\left ( s',a' \right )-q_{t-1}\left ( s,a \right ) \right ] $$

从形式上看，TD也需要采样过程；从更新过程上看，TD法通过下一时刻的价值更新前一时刻的价值，似乎是承认了下一时刻的回报和价值足够优秀，从而利用了最优子结构进行更新，这和动态规划的思想是一样的。
TD系列的方法更新公式只考虑了当前一步的回报值，其余的计算均使用了之前的估计值，所以在整体系统没有达到最优的，这样的估计都是有偏差的，由于只估计了一步，所以它在估计值方面受到的波动比较小，因此方差也会减少许多。

## 1.4.1 On-Policy算法（完全依据交互序列）

上文叙述的算法是SARSA算法，S：当前状态，A：当前行动，R：模拟得到的奖励，S：模拟进入的下一个状态，A：模拟中采取的下一个行动。核心公式就是：

$$ q^{T}\left ( s,a \right )=q^{T-1}\left ( s,a \right )+\frac{1}{N}\left [ r(s')+q^{T-1}\left ( s',a' \right )-q^{T-1}\left ( s,a \right ) \right ] $$

## 1.4.2 Off-Policy算法（不完全依据交互序列）

另外一种方法是Q-Learning算法，其核心公式：

$$ q^{T}\left ( s,a \right )=q^{T-1}\left ( s,a \right )+\frac{1}{N}\left [ r(s')+max_{a'}q^{T-1}\left ( s',a' \right )-q^{T-1}\left ( s,a \right ) \right ] $$

两个算法的区别在于SARSA算法遵循交互序列，根据下一步的真实行动进行价值估计；Q-Learning算法没有遵循交互序列，而在下一时刻选择了使价值最大的行动。
## 1.5 Deep Q Network

整体思路：在使用Q-Learning作为优化算法的前提下，使用深层神经网络表示值函数，直接使用游戏图像和得分进行训练，不再使用其他领域的知识。
### 1.5.1 数据预处理

将原始图像转化为84*84维的灰度图，收集从当前时刻起的前N帧画面，并将这些信息结合起来作为模型的输入。实验中N被设置为4。另外，游戏的得分也要作预处理，将不同游戏的得分都压缩到\[-1, 1\]之间。

### 1.5.2 环境交互

ε-greedy的策略，一开始策略以100%的概率随机产生行动，随着训练的进行，概率不断衰减，最终衰减至10%，也就是说有90%的概率执行当前最优策略。这样以探索为主的策略转换为以利用为主的策略。

### 1.5.3 模型结构

模型输出是一个长度为\|A\|的向量，向量中的每一个值表示对应行动的价值估计，这样只需要计算一次就可以求出所有行动的价值，无论行动有多少，我们评估价值的时间是一样的。模型主体采用的是卷积神经网络。三层卷积网络和一层全连接层。

### 1.5.4 Random Start

为了增强探索性同时不至于使模型效果变糟，我们可以设定在游戏开始很短的一段时间内，让Agent执行随机的行动。
### 1.5.5 Frame-Skipping

每隔一定帧数执行一次行动，例如每隔四帧进行一次行动选择。
### 1.5.6 Replay Buffer

> 交互得到的序列存在一定的相关性；
> 交互数据的使用效率。采用ReplayBuffer保存交互的样本信息，保存当前状态s、行动a和长期累计回报v。
Replay Buffer包括两个过程：收集样本和采集样本。Replay Buffer会在缓存中均匀地随机采样进行学习。

### 1.5.7 Target Network

算法分成如下两个步骤：
	计算当前状态行动下的价值目标值：

$$ \Delta q\left ( s,a \right )=r(s')+max_{a'}q^{T-1}\left ( s',a' \right ) $$

网络模型的更新

$$ q^{T}\left ( s,a \right )=q^{T-1}\left ( s,a \right )+\frac{1}{N}\left [ \Delta q\left ( s,a \right )-q^{T-1}\left ( s,a \right ) \right ] $$

为了避免模型更新带来的波动，引入目标网络，而原本的模型被称为表现模型（Behavior Network），训练过程如下：
- 训练开始时，两个模型使用完全相同的参数；
- 在训练过程中，Behavior Network负责与环境交互，得到交互样本；
- 在学习过程中，由Q-Learning得到的目标价值由Target Network得到，然后用它与Behavior Network的估计值进行比较得出目标值并更新Behavior Network。
- 每当训练完成一定轮数的迭代，Behavior Network模型的参数就会同步给Target Network，进行下一阶段的学习。

通过Target Network，计算目标价值的模型在一定时间内将被固定，这样模型可以减轻波动性。

# 2. DQN的改进算法
## 2.1 Double Q-Learning
Q-Learning在计算时利用了下一时刻的最优价值，模型在选择最优行动和计算目标值时依然使用了同样的参数模型，这样必然会造成对价值得过高估计，为了尽可能减少过高估计得影响，一个简单的办法是把选择最优行动和估计最高行动两部分的工作分离。我们用Behavior Network完成最优行动的选择工作，通过这样的变化，算法在三个环节的模型安排如下：
- 采样：Behavior NetworkQ(θ)
- 选择最优行动：Behavior NetworkQ(θ)
- 计算目标价值：Target NetworkQ(θ)

## 2.2 Priority Replay Buffer
Priority Replay Buffer根据模型对当前模型的表现情况，给样本一定的权重，在采样时候样本被采样的概率就和这个权重有关。交互时表现得越差，对应的权重越高。与Replay Buffer相比，差别在于：
> 为每一个存入Replay Buffer的样本设定一个权重；
> 使用这个权重完成采样过程，采用线段二叉树加速权重修改和集合采样；

如何定义有价值？TD-Error；估计值和目标值的差距越大，样本越有价值。由于随着训练的过程，这个值有所变化。采用其他方式辅助计算，公式为：
$P(i)=\frac{p_{i}^{\infty }}{\sum_{k}p_{k}^{\infty }}$其中： $p_{i}$ 是TD-Error，$\alpha$可以调整TD-Error的重要性。
> 提供了一个参数用于调整每个样本对模型更新的影响。

为了让更新无偏，采用重要性采样。
 
新的权重更新公式为：$w_{i}=\left ( \frac{1}{N\cdot P_{PRB}(i)} \right )^{\beta}$
 
算法步骤总结：
- 在样本存入replay Buffer时，计算

$$ P(i)=\frac{p_{i}^{\infty }}{\sum_{k}p_{k}^{\infty }} $$

- 在样本取出时，以第一步计算得到的概率进行采样；

- 在更新时，为每一个样本添加 $$ w_{i}=\left ( \frac{1}{N\cdot P_{PRB}(i)} \right )^{\beta} $$ 的权重

- 随着训练的进行，让 从某个小于1的值渐进地靠近1

## 2.3 Dueling DQN
Dueling DQN主要突破点在于利用模型结构将值函数表示成更细致的形式：

$$ q(s_{t},a_{t})=v(s_{t})+A(s_{t},a_{t}) $$

其中：A是优势函数（Advantage Function），可以用来表示当前行动和平均表现之间的区别：如果优于平均表现，则优势函数为正，反之则为负。另外A函数的期望值为0：
 
因此就可以对输出的A值进行约束，将公式变为：

$$ E_{a}[A(s_{t},a_{t})]=E_{a}[q(s_{t},a_{t}-v(s)]=v_{s}-v=0 $$

## 2.4 解决DQN的冷启动问题
Deep Q-Learning from DemonDemonstrations为我们探索了强化学习和监督学习结合的一种方案。主要思路是利用预先准备好的优质采样轨迹加快模型前期的训练速度。除了用监督学习完成与环境交互前的预训练，还要用强化学习完成 DQN原本方法模型中的训练。最终目标函数变为：

$$ J(q)=J_{DQ}(q)+\lambda_{1}J_{n}(q)+\lambda_{2}J_{E}(q)+\lambda_{3}J_{L_{2}}(q) $$

## 2.5Distributional DQN
## 2.6Noisy Network
## 2.7Rainbow
未完待续……