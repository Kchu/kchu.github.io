---
layout: post
title:  "基于MDP的策略学习算法"
date:   2019-01-20 18:37:56 +0800
<!-- categories: RL -->
tags: MDP, 强化学习
---

* content
{:toc}

<!-- <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> -->

### 1. 策略迭代
强化学习就是找到最优的策略，使每一个状态的价值最大化，相当于求解：

$$\large \pi^{*}=argmax_{\pi}v_{\pi}\left( x \right )$$
$$\large a^{*}=argmax_{a}q_{\pi}\left ( x,a \right )$$

（1）以某种策略开始，计算当前策略下的值函数
（2）利用这个值函数，找到更好的值函数
（3）用这个策略继续前行，更新值函数，然后不断迭代。
- **优化算法**
（1）计算当前策略下的值函数估计：

$$\large V\left ( s_{t} \right )=\sum_{a}\pi\left ( a|s_{t} \right )\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t} \right )\left [ R\left (  s_{t}  \right) + \gamma V\left ( s_{t+1} \right )\right ) ]$$

　　矩阵法计算复杂度较高，故采用迭代方法计算，迭代公式变成：
　　
$$\large V^{T}\left ( s_{t} \right )=\sum_{a}\pi\left ( a|s_{t} \right )\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t} \right )\left [ R\left (  R\left (  s_{t}  \right) + \gamma V^{T-1}\left ( s_{t+1} \right )\right  ) \right ]$$

（2）通过该值函数计算状态-行为值函数，然后根据同一状态下的行为价值更新策略，完成状态的更新。

$$\large q\left ( s_{t},a_{t} \right )=\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t},a_{t} \right )\left [ r_{t+1}+ \gamma v\left (s_{t+1}  \right ) \right ]$$

（3）继续迭代更新，重复步骤（1）,（2）。

### 2. 价值迭代
　　价值迭代看起来似乎是“贪心”版的策略迭代算法。价值迭代的中心是值函数，它通过动态规划的方法迭代更新函数，并最终求出策略函数。
　　状态值函数更新方法：
$$\large \pi^{*}\left ( x \right )=arg max_{a}\sum_{s_{t+1}}p\left ( s_{t+1}|s_{t},a_{t} \right )\left [ r_{t+1}+\gamma v\left ( s_{t+1} \right ) \right ] $$
　　由于最优策略的存在，实际上策略最终的选择是单一的。也就是说，对于每一个状态，最优策略会采取某一种行动，这种行动不会比其他行动差，所以我们可以得到状态值函数更新的方法：
$$\large v\left ( x \right ) \leftarrow  max_{a}\sum_{s'}p\left ( s'|s,a \right )\left [ r\left ( s,a,s' \right )+\gamma \tilde{v} \left ( s' \right ) \right ]$$
<div STYLE="page-break-after: always;"></div>
这样来更新状态值函数，用到了最优子结构的性质。
### 3. 泛化迭代
　　策略迭代方法的中心是策略函数，它通过反复执行“策略评估+策略提升”两个步骤使策略变得越来越好；价值迭代法的中心是值函数，它通过利用动态规划法迭代值函数，并最终求出策略函数。
　　广义策略迭代就是定义一个迭代算法簇，其中的算法都是由策略迭代和价值迭代算法组合而成的。
***
